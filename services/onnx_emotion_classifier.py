# ONNX Runtime ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ì„œë¹„ìŠ¤
# - PyTorch ëŒ€ì‹  ONNX Runtime ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
# - ë¡œì»¬ onnx/ í´ë”ì˜ ëª¨ë¸ íŒŒì¼ ì‚¬ìš©

from __future__ import annotations
from typing import Iterable, List, Tuple
from threading import Lock
import os
import json
import numpy as np
import subprocess
import urllib.request
import logging

FILES = ["model.onnx", "model.int8.onnx", "config.json", "tokenizer.json"]

from core.config import settings
from schemas.emotions import ECRefinedSegment, ClassifiedSegment

# ê°ì • ë¼ë²¨ ë§¤í•‘ (ëª¨ë¸ ì¶œë ¥ì— ë§ê²Œ ì¡°ì • í•„ìš”)
LABEL_KO_MAP = {
    "anger": "ë¶„ë…¸",
    "anxiety": "ë¶ˆì•ˆ", 
    "disgust": "í˜ì˜¤",
    "fear": "ë‘ë ¤ì›€",
    "sadness": "ìŠ¬í””",
    "joy": "ê¸°ì¨",
    "surprise": "ë†€ëŒ",
    "neutral": "ì¤‘ë¦½",
    "embarrassment": "ë‹¹í™©",
    "confusion": "ë‹¹í™©",
    "hurt": "ìƒì²˜",
    "etc": "ê¸°íƒ€",
    "others": "ê¸°íƒ€",
}

def _normalize_label(label: str, id2label: dict[int, str]) -> str:
    """ëª¨ë¸ì´ ë°˜í™˜í•˜ëŠ” ë¼ë²¨ì„ í•œêµ­ì–´ë¡œ ì •ê·œí™”"""
    raw = label
    # ìˆ«ì ë¼ë²¨ ì²˜ë¦¬
    if raw.isdigit():
        idx = int(raw)
        if idx in id2label:
            raw = id2label[idx]
    # LABEL_3 ê°™ì€ í¬ë§· ì²˜ë¦¬
    if raw.startswith("LABEL_") and raw[6:].isdigit():
        idx = int(raw[6:])
        if idx in id2label:
            raw = id2label[idx]
    # ì˜ë¬¸ ë¼ë²¨ ë§¤í•‘
    low = raw.strip().lower()
    if low in LABEL_KO_MAP:
        return LABEL_KO_MAP[low]
    return raw

# ëª¨ë¸ ì‹±ê¸€í†¤ ë³´ì¥
_onnx_model_singleton = None
_onnx_model_lock = Lock()

class ONNXEmotionClassifier:
    """ONNX Runtime ê¸°ë°˜ ê°ì • ë¶„ë¥˜ê¸°"""
    
    def __init__(self, onnx_path: str, config_path: str, tokenizer_path: str):
        import onnxruntime as ort
        
        # ONNX ëª¨ë¸ ë¡œë”© (ì–‘ìí™”ëœ ëª¨ë¸ ìµœì í™”)
        session_options = ort.SessionOptions()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        session_options.intra_op_num_threads = 1
        session_options.inter_op_num_threads = 1
        
        # ì–‘ìí™”ëœ ëª¨ë¸ì¸ì§€ í™•ì¸
        is_int8 = 'int8' in onnx_path.lower()
        if is_int8:
            print("ğŸ”§ int8 ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©")
            session_options.enable_cpu_mem_arena = True
            session_options.enable_mem_pattern = True
            session_options.enable_mem_reuse = True
        
        self.session = ort.InferenceSession(
            onnx_path,
            sess_options=session_options,
            providers=['CPUExecutionProvider']  # CPUë§Œ ì‚¬ìš©
        )
        
        # ëª¨ë¸ ì„¤ì • ë¡œë”©
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        from tokenizers import Tokenizer
        self.tokenizer = Tokenizer.from_file(tokenizer_path)
        
        # ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ì •ë³´
        self.input_names = [inp.name for inp in self.session.get_inputs()]
        self.output_name = self.session.get_outputs()[0].name
        
        # ë¼ë²¨ ë§¤í•‘
        self.id2label = self.config.get('id2label', {})
        
        # ëª¨ë¸ íƒ€ì… í™•ì¸ ë° ì¶œë ¥
        model_type = "int8 (ì–‘ìí™”)" if 'int8' in onnx_path.lower() else "fp32 (ê¸°ë³¸)"
        print(f"âœ… ONNX ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {onnx_path}")
        print(f"ğŸ” ëª¨ë¸ íƒ€ì…: {model_type}")
        print(f"ğŸ“Š ëª¨ë¸ ì…ë ¥: {self.input_names}")
        print(f"ğŸ“Š ëª¨ë¸ ì¶œë ¥: {self.output_name}")
        
        # ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš© ì‹œ ì¶”ê°€ ì •ë³´
        if 'int8' in onnx_path.lower():
            print("ğŸ’¡ int8 ëª¨ë¸: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì¶”ë¡  ì†ë„ í–¥ìƒ")
    
    def tokenize(self, text: str, max_length: int = 128) -> dict:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜"""
        encoding = self.tokenizer.encode(text)
        
        # input_ids
        input_ids = encoding.ids[:max_length]
        # íŒ¨ë”©ìœ¼ë¡œ max_length ë§ì¶”ê¸°
        if len(input_ids) < max_length:
            input_ids += [0] * (max_length - len(input_ids))
        
        # attention_mask (1: ì‹¤ì œ í† í°, 0: íŒ¨ë”©)
        attention_mask = [1] * len(encoding.ids[:max_length])
        if len(attention_mask) < max_length:
            attention_mask += [0] * (max_length - len(attention_mask))
        
        # token_type_ids (BERT ìŠ¤íƒ€ì¼, ëª¨ë‘ 0)
        token_type_ids = [0] * max_length
        
        return {
            'input_ids': np.array([input_ids], dtype=np.int64),
            'attention_mask': np.array([attention_mask], dtype=np.int64),
            'token_type_ids': np.array([token_type_ids], dtype=np.int64)
        }
    
    def predict_one(self, text: str, max_length: int = 128) -> Tuple[str, float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì • ì˜ˆì¸¡"""
        if not text or not text.strip():
            return "ì¤‘ë¦½", 0.0
        
        try:
            # í† í°í™”
            tokenized = self.tokenize(text, max_length)
            
            # ëª¨ë“  ì…ë ¥ì„ ëª¨ë¸ì— ì œê³µ
            model_inputs = {
                'input_ids': tokenized['input_ids'],
                'attention_mask': tokenized['attention_mask'],
                'token_type_ids': tokenized['token_type_ids']
            }
            
            # ONNX ì¶”ë¡ 
            outputs = self.session.run(
                [self.output_name], 
                model_inputs
            )
            
            # ê²°ê³¼ ì²˜ë¦¬
            logits = outputs[0][0]  # ì²« ë²ˆì§¸ ë°°ì¹˜, ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤
            probabilities = self._softmax(logits)
            
            # ìµœê³  í™•ë¥ ì˜ ë¼ë²¨ ì„ íƒ
            predicted_id = np.argmax(probabilities)
            confidence = float(probabilities[predicted_id])
            
            # ë¼ë²¨ ì •ê·œí™”
            label = self.id2label.get(str(predicted_id), str(predicted_id))
            normalized_label = _normalize_label(label, self.id2label)
            
            return normalized_label, confidence
            
        except Exception as e:
            print(f"âš ï¸ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return "ì¤‘ë¦½", 0.0
    
    def predict(self, texts: Iterable[str], max_length: int = 128) -> List[Tuple[str, float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì • ì˜ˆì¸¡"""
        return [self.predict_one(t, max_length) for t in texts]
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

def _pick_onnx_root() -> str:
    """
    ONNX ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„ íƒ & ìƒì„±
    ìš°ì„ ìˆœìœ„:
      1) /app/onnx (ì»¨í…Œì´ë„ˆ/Render í™˜ê²½ì—ì„œ ê°€ì¥ ì•ˆì „)
      2) í™˜ê²½ë³€ìˆ˜ ONNX_MODEL_PATH
      3) settings.BASE_DIR/onnx (ë¡œì»¬ ê°œë°œ)
      4) services ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ ../onnx (ìµœí›„)
    """
    candidates = [
        "/app/onnx",
        os.getenv("ONNX_MODEL_PATH"),
        os.path.join(getattr(settings, "BASE_DIR", os.getcwd()), "onnx"),
        os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "onnx")),
    ]

    for candidate in candidates:
        if not candidate:
            continue
        try:
            os.makedirs(candidate, exist_ok=True)
            return candidate
        except Exception:
            # ê¶Œí•œ ë¬¸ì œ ë“±ìœ¼ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë‹ˆ ë‹¤ìŒ í›„ë³´ë¡œ
            continue

    # ì •ë§ ëª¨ë“  í›„ë³´ê°€ ì‹¤íŒ¨í•˜ë©´, í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì˜ onnxë¡œ
    fallback = os.path.join(os.getcwd(), "onnx")
    os.makedirs(fallback, exist_ok=True)
    return fallback


def _download_if_missing(onnx_dir: str):
    """
    ONNX_ZIP_URLì€ 'ë² ì´ìŠ¤ URL'ì´ì–´ì•¼ í•¨.
    ì˜ˆ) https://github.com/Seoyoung0519/LipSee/releases/download/v1.0.0
    ìœ„ BASE_URL/<íŒŒì¼ëª…> ìœ¼ë¡œ FILES ë°°ì—´ì„ ê°œë³„ ë‹¤ìš´ë¡œë“œí•œë‹¤.
    """
    base = os.getenv("ONNX_ZIP_URL", "").rstrip("/")
    if not base:
        logging.warning("ONNX_ZIP_URL ë¹„ì–´ ìˆìŒ â€” ìë™ ë‹¤ìš´ë¡œë“œ ìƒëµ")
        return

    for fname in FILES:
        url = f"{base}/{fname}"
        dst = os.path.join(onnx_dir, fname)
        if os.path.isfile(dst):
            logging.info(f"skip: {dst} ì´ë¯¸ ì¡´ì¬")
            continue

        try:
            logging.info(f"DL {url} -> {dst}")
            urllib.request.urlretrieve(url, dst)
        except Exception as e:
            logging.exception(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url} -> {dst} ({e})")
            # ìµœì†Œí•œ model.onnxëŠ” ë°˜ë“œì‹œ í•„ìš”í•˜ë¯€ë¡œ, ê·¸ê²Œ ì‹¤íŒ¨í•˜ë©´ ì¦‰ì‹œ ì—ëŸ¬
            if fname == "model.onnx":
                raise


def _get_onnx_model() -> ONNXEmotionClassifier:
    """ONNX ëª¨ë¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _onnx_model_singleton
    
    if _onnx_model_singleton is None:
        with _onnx_model_lock:
            if _onnx_model_singleton is None:
                # === [CHANGE] ê²½ë¡œ ì„ íƒ + ìë™ ë‹¤ìš´ë¡œë“œ ë³´ì¥ ====================
                # í•­ìƒ /app/onnx ìš°ì„  ìƒì„±/ì‚¬ìš© â†’ ì—†ìœ¼ë©´ ë§Œë“¤ì–´ ì“°ê³ , ì—†ìœ¼ë©´ ë°›ëŠ”ë‹¤
                onnx_dir = _pick_onnx_root()

                print(f"ğŸ” ìµœì¢… ONNX ëª¨ë¸ ê²½ë¡œ: {onnx_dir}")
                print(f"ğŸ” í•´ë‹¹ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(onnx_dir)}")

                # model.onnxê°€ ì—†ë‹¤ë©´, ë¦´ë¦¬ìŠ¤ì—ì„œ ì¦‰ì‹œ ë‚´ë ¤ë°›ê¸°
                if not os.path.isfile(os.path.join(onnx_dir, "model.onnx")):
                    print("ğŸ“¥ model.onnxì´ ì—†ì–´ ìë™ ë‹¤ìš´ë¡œë“œ ì‹œë„")
                    _download_if_missing(onnx_dir)

                # ë””ë²„ê·¸ ì¶œë ¥
                try:
                    print("ğŸ” ë””ë ‰í† ë¦¬ ëª©ë¡:")
                    result = subprocess.run(['ls', '-la', onnx_dir], capture_output=True, text=True)
                    print(result.stdout)
                except Exception as e:
                    print(f"í´ë” ë‚´ìš© í™•ì¸ ì‹¤íŒ¨: {e}")

                int8_path = os.path.join(onnx_dir, 'model.int8.onnx')
                fp32_path = os.path.join(onnx_dir, 'model.onnx')

                if os.path.exists(int8_path):
                    onnx_path = int8_path
                    print(f"ğŸš€ ì–‘ìí™”ëœ int8 ëª¨ë¸ ì‚¬ìš©: {int8_path}")
                else:
                    onnx_path = fp32_path
                    print(f"ğŸ“Š ê¸°ë³¸ fp32 ëª¨ë¸ ì‚¬ìš©: {fp32_path}")

                config_path = os.path.join(onnx_dir, 'config.json')
                tokenizer_path = os.path.join(onnx_dir, 'tokenizer.json')

                # ìµœì¢… ì¡´ì¬ ê²€ì¦(ì‹¤íŒ¨ ì‹œ íŒŒì¼ ëª©ë¡ í¬í•¨í•˜ì—¬ ì—ëŸ¬)
                missing = [p for p in [onnx_path, config_path, tokenizer_path] if not os.path.exists(p)]
                if missing:
                    try:
                        listing = ", ".join(os.listdir(onnx_dir))
                    except Exception:
                        listing = "(dir unreadable)"
                    raise FileNotFoundError(
                        f"ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                        f"- base dir: {onnx_dir}\n"
                        f"- missing: {missing}\n"
                        f"- files: {listing}\n"
                        f"- ONNX_ZIP_URL={os.getenv('ONNX_ZIP_URL','')}"
                    )
                # ===============================================================

                _onnx_model_singleton = ONNXEmotionClassifier(onnx_path, config_path, tokenizer_path)

    return _onnx_model_singleton

def classify_segments(
    segs: Iterable[ECRefinedSegment],
    add_prefix: bool = True
) -> List[ClassifiedSegment]:
    """ECRefinedSegment ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê°ì • ë¶„ë¥˜ í›„ ClassifiedSegmentë¡œ ë³€í™˜"""
    model = _get_onnx_model()
    preds = model.predict([s.corrected for s in segs])
    
    out: List[ClassifiedSegment] = []
    for s, (emo, score) in zip(segs, preds):
        final_text = f"({emo}) {s.corrected}" if add_prefix else s.corrected
        out.append(ClassifiedSegment(
            id=s.id,
            start=s.start,
            end=s.end,
            original=s.original,
            picked_candidate=s.picked_candidate,
            gain=s.gain,
            corrected=s.corrected,
            emotion=emo,
            score=score,
            final_text=final_text
        ))
    return out
