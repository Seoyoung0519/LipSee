# ONNX Runtime ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ì„œë¹„ìŠ¤ (PyTorch ëŒ€ì‹ )
# - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ì ˆì•½: PyTorch(~800MB) â†’ ONNX Runtime(~50MB)
# - ë¡œì»¬ onnx/ í´ë”ì˜ ëª¨ë¸ íŒŒì¼ ì‚¬ìš©

# ONNX Runtime ê¸°ë°˜ ê°ì • ë¶„ë¥˜ê¸° ì§ì ‘ êµ¬í˜„
from __future__ import annotations
from typing import Iterable, List, Tuple
from threading import Lock
import os
import json
import numpy as np

from core.config import settings
from schemas.emotions import ECRefinedSegment, ClassifiedSegment

# ê°ì • ë¼ë²¨ ë§¤í•‘ (ì›ë˜ ëª¨ë¸ì˜ ì‹¤ì œ ì¶œë ¥)
LABEL_KO_MAP = {
    # ì›ë˜ ëª¨ë¸ì˜ 11ê°€ì§€ ê°ì •
    "joy": "ê¸°ì¨(í–‰ë³µí•œ)",
    "grateful": "ê³ ë§ˆìš´",
    "excited": "ì„¤ë ˆëŠ”(ê¸°ëŒ€í•˜ëŠ”)",
    "loving": "ì‚¬ë‘í•˜ëŠ”",
    "fun": "ì¦ê±°ìš´(ì‹ ë‚˜ëŠ”)",
    "daily": "ì¼ìƒì ì¸",
    "thoughtful": "ìƒê°ì´ ë§ì€",
    "sadness": "ìŠ¬í””(ìš°ìš¸í•œ)",
    "difficult": "í˜ë“¦(ì§€ì¹¨)",
    "annoyed": "ì§œì¦ë‚¨",
    "worried": "ê±±ì •ìŠ¤ëŸ¬ìš´(ë¶ˆì•ˆí•œ)",
    
    # ê¸°íƒ€ ê°€ëŠ¥í•œ ë§¤í•‘
    "neutral": "ì¼ìƒì ì¸",
    "anger": "ì§œì¦ë‚¨",
    "anxiety": "ê±±ì •ìŠ¤ëŸ¬ìš´(ë¶ˆì•ˆí•œ)",
    "fear": "ê±±ì •ìŠ¤ëŸ¬ìš´(ë¶ˆì•ˆí•œ)",
    "surprise": "ì„¤ë ˆëŠ”(ê¸°ëŒ€í•˜ëŠ”)",
    "embarrassment": "ìƒê°ì´ ë§ì€",
    "confusion": "ìƒê°ì´ ë§ì€",
    "hurt": "ìŠ¬í””(ìš°ìš¸í•œ)",
    "etc": "ì¼ìƒì ì¸",
    "others": "ì¼ìƒì ì¸",
}

def _normalize_label(label: str, id2label: dict[int, str]) -> str:
    """ëª¨ë¸ì´ ë°˜í™˜í•˜ëŠ” ë¼ë²¨ì„ í•œêµ­ì–´ë¡œ ì •ê·œí™”"""
    raw = label
    if raw.isdigit():
        idx = int(raw)
        if idx in id2label:
            raw = id2label[idx]
    if raw.startswith("LABEL_") and raw[6:].isdigit():
        idx = int(raw[6:])
        if idx in id2label:
            raw = id2label[idx]
    low = raw.strip().lower()
    if low in LABEL_KO_MAP:
        return LABEL_KO_MAP[low]
    return raw

# ëª¨ë¸ ì‹±ê¸€í†¤ ë³´ì¥
_onnx_model_singleton = None
_onnx_model_lock = Lock()

class ONNXEmotionClassifier:
    """ONNX Runtime ê¸°ë°˜ ê°ì • ë¶„ë¥˜ê¸°"""
    
    def __init__(self, onnx_path: str, config_path: str, tokenizer_path: str):
        import onnxruntime as ort
        
        # ONNX ëª¨ë¸ ë¡œë”©
        self.session = ort.InferenceSession(
            onnx_path,
            providers=['CPUExecutionProvider']
        )
        
        # ëª¨ë¸ ì„¤ì • ë¡œë”©
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        from tokenizers import Tokenizer
        self.tokenizer = Tokenizer.from_file(tokenizer_path)
        
        # ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ì •ë³´
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        
        # ë¼ë²¨ ë§¤í•‘
        self.id2label = self.config.get('id2label', {})
        
        print(f"âœ… ONNX ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {onnx_path}")
        print(f"ğŸ“Š ëª¨ë¸ ì…ë ¥: {self.input_name}")
        print(f"ğŸ“Š ëª¨ë¸ ì¶œë ¥: {self.output_name}")
        print(f"ğŸ“Š ì„¤ì • íŒŒì¼: {config_path}")
        print(f"ğŸ“Š í† í¬ë‚˜ì´ì €: {tokenizer_path}")
        print(f"ğŸ“Š ë¼ë²¨ ë§¤í•‘: {self.id2label}")
        print(f"ğŸ“Š ëª¨ë¸ ì…ë ¥ í˜•íƒœ: {[inp.shape for inp in self.session.get_inputs()]}")
        print(f"ğŸ“Š ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {[out.shape for out in self.session.get_outputs()]}")
    
    def tokenize(self, text: str, max_length: int = 128) -> dict:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜"""
        encoding = self.tokenizer.encode(text)
        
        # input_ids
        input_ids = encoding.ids[:max_length]
        # íŒ¨ë”©ìœ¼ë¡œ max_length ë§ì¶”ê¸°
        if len(input_ids) < max_length:
            input_ids += [0] * (max_length - len(input_ids))
        
        # attention_mask (1: ì‹¤ì œ í† í°, 0: íŒ¨ë”©)
        attention_mask = [1] * len(encoding.ids[:max_length])
        if len(attention_mask) < max_length:
            attention_mask += [0] * (max_length - len(attention_mask))
        
        # token_type_ids (BERT ìŠ¤íƒ€ì¼, ëª¨ë‘ 0)
        token_type_ids = [0] * max_length
        
        return {
            'input_ids': np.array([input_ids], dtype=np.int64),
            'attention_mask': np.array([attention_mask], dtype=np.int64),
            'token_type_ids': np.array([token_type_ids], dtype=np.int64)
        }
    
    def predict_one(self, text: str, max_length: int = 128) -> Tuple[str, float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì • ì˜ˆì¸¡"""
        if not text or not text.strip():
            return "ì¤‘ë¦½", 0.0
        
        try:
            print(f"ğŸ” ì…ë ¥ í…ìŠ¤íŠ¸: {text}")
            tokenized = self.tokenize(text, max_length)
            print(f"ğŸ” í† í°í™” ê²°ê³¼: input_ids={tokenized['input_ids'].shape}, attention_mask={tokenized['attention_mask'].shape}")
            
            # ëª¨ë“  ì…ë ¥ì„ ëª¨ë¸ì— ì œê³µ
            model_inputs = {
                'input_ids': tokenized['input_ids'],
                'attention_mask': tokenized['attention_mask'],
                'token_type_ids': tokenized['token_type_ids']
            }
            
            outputs = self.session.run([self.output_name], model_inputs)
            print(f"ğŸ” ëª¨ë¸ ì¶œë ¥: {len(outputs)}, í˜•íƒœ: {[o.shape for o in outputs]}")
            
            logits = outputs[0][0]
            print(f"ğŸ” ë¡œì§“ í˜•íƒœ: {logits.shape}, ê°’: {logits[:5]}...")
            
            probabilities = self._softmax(logits)
            print(f"ğŸ” í™•ë¥  í˜•íƒœ: {probabilities.shape}, ê°’: {probabilities[:5]}...")
            
            predicted_id = np.argmax(probabilities)
            confidence = float(probabilities[predicted_id])
            
            print(f"ğŸ” ì˜ˆì¸¡ ID: {predicted_id}, ì‹ ë¢°ë„: {confidence:.3f}")
            
            label = self.id2label.get(str(predicted_id), str(predicted_id))
            print(f"ğŸ” ì›ë³¸ ë¼ë²¨: {label}")
            
            normalized_label = _normalize_label(label, self.id2label)
            print(f"ğŸ” ì •ê·œí™”ëœ ë¼ë²¨: {normalized_label}")
            
            return normalized_label, confidence
            
        except Exception as e:
            print(f"âš ï¸ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return "ì¤‘ë¦½", 0.0
    
    def predict(self, texts: Iterable[str], max_length: int = 128) -> List[Tuple[str, float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì • ì˜ˆì¸¡"""
        return [self.predict_one(t, max_length) for t in texts]
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

def _get_onnx_model() -> ONNXEmotionClassifier:
    """ONNX ëª¨ë¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _onnx_model_singleton
    
    if _onnx_model_singleton is None:
        with _onnx_model_lock:
            if _onnx_model_singleton is None:
                # ONNX ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì–‘ìí™”ëœ ëª¨ë¸ ìš°ì„  ì‚¬ìš©)
                onnx_dir = os.path.join(os.path.dirname(__file__), '..', 'onnx')
                
                # ì–‘ìí™”ëœ ëª¨ë¸ ìš°ì„  ì‹œë„
                int8_path = os.path.join(onnx_dir, 'model.int8.onnx')
                if os.path.exists(int8_path):
                    onnx_path = int8_path
                    print(f"ğŸš€ ì–‘ìí™”ëœ int8 ëª¨ë¸ ì‚¬ìš©: {int8_path}")
                else:
                    onnx_path = os.path.join(onnx_dir, 'model.onnx')  # ì›ë³¸ ëª¨ë¸
                    print(f"ğŸ“Š ê¸°ë³¸ fp32 ëª¨ë¸ ì‚¬ìš©: {onnx_path}")
                
                config_path = os.path.join(onnx_dir, 'config.json')
                tokenizer_path = os.path.join(onnx_dir, 'tokenizer.json')
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not all(os.path.exists(p) for p in [onnx_path, config_path, tokenizer_path]):
                    raise FileNotFoundError(f"ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {onnx_dir}")
                
                # ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶œë ¥
                model_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)
                print(f"ğŸ“Š ì‚¬ìš©í•  ëª¨ë¸: {os.path.basename(onnx_path)} ({model_size_mb:.1f}MB)")
                
                _onnx_model_singleton = ONNXEmotionClassifier(
                    onnx_path, config_path, tokenizer_path
                )
    
    return _onnx_model_singleton

def classify_segments(
    segs: Iterable[ECRefinedSegment],
    add_prefix: bool = True
) -> List[ClassifiedSegment]:
    """ECRefinedSegment ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê°ì • ë¶„ë¥˜ í›„ ClassifiedSegmentë¡œ ë³€í™˜"""
    model = _get_onnx_model()
    preds = model.predict([s.corrected for s in segs])
    
    out: List[ClassifiedSegment] = []
    for s, (emo, score) in zip(segs, preds):
        final_text = f"({emo}) {s.corrected}" if add_prefix else s.corrected
        out.append(ClassifiedSegment(
            id=s.id, start=s.start, end=s.end,
            original=s.original, picked_candidate=s.picked_candidate,
            gain=s.gain, corrected=s.corrected,
            emotion=emo, score=score, final_text=final_text
        ))
    return out
